![image-20220908183220654](D:\论文\截图\image-20220908183220654.png)

当batch size等于1000时，里面大部分图片是很相似的图片，重复的图片就是在重复计算。影响收敛精度。

![image-20220908183533358](D:\论文\截图\image-20220908183533358.png)

==（重要）答==

看你怎么训练，当我去训练f(x)

当我发现g(x)很不好很难训练

==他就会拿不到梯度==

因为这个模型是我训练的，如果我的模型发现我直接拿x效果本身就已经很好了（即比在加上后面训练的一个新的g（x）好）

加上一个g（x）对于我loss的下降没有什么明显的帮助时

在做梯度反传时，g（x）就不会有什么梯度，g（x）的权重也就不会被更新

g（x）就会是一些很小的随机权重，对模型预测不会产生什么贡献

甚至g（x）的梯度会变成0，其权重也会变的很小



这也就是为什么ResNet在加深时候通常不会让模型变坏。



![image-20220908184208602](D:\论文\截图\image-20220908184208602.png)

![image-20220908184253068](D:\论文\截图\image-20220908184253068.png)

纵坐标：学习率

横坐标：时间

红：固定学习率

蓝：step学习率

绿：cos学习率

好处：前面足够快，后面学习率小了有足够时间去做微调

比较简单，只要一个最大值一个最小值，而不是像step一样要多少下之类的

![image-20220908184600871](D:\论文\截图\image-20220908184600871.png)

==（重要）答==

我在fit这个函数的时候，在f（x）式子中的 x 是一个小模型，我在fit的时候，我会先fit里面这个小模型 x 。

如果  x  作为底层已经训练的差不多了之后，再去训练g（x）来提高模型的精度

例：

![image-20220908185832490](D:\论文\截图\image-20220908185832490.png)

假设要训练这样一个曲线

![image-20220908185854307](D:\论文\截图\image-20220908185854307.png)

那么我先拟合一个蓝线这样的模型，比如这个蓝线就是我现在==layer1==做的事情

![image-20220908190256039](D:\论文\截图\image-20220908190256039.png)

然后layer2在layer1的基础上调整



ResNet就会先去训练一些比较下层的模型

==比如我训练一个ResNet152，那么从直观来说，他会先训练相对于下层的ResNet18==

因为x是作为一个项加入到里面的，所以就是相当于训练了ResNet18，没有少全局平均池化层。每次先拟合f（x）中那个x项

然后剩下没有fit好的东西上层再去fit



![image-20220908190601498](D:\论文\截图\image-20220908190601498.png)

解包，即把block中每个层解包了按顺序一层层放到nn.Sequential中

![image-20220908190710033](D:\论文\截图\image-20220908190710033.png)

==（重要）答==

两个BN参数不一样，每个BN有自己的参数要学，所以不能像ReLU一样只定义一个，每个BN有自己的参数

==inplace参数==：正常情况下，ReLU新建一个output，把输入做max0（==跟0比大小，输出大的==）（即ReLU的操作）换过去

inplace=true就是我不新建一个output了，改写input把值替换掉