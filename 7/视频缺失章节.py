'''
视频缺失章节
'''

'''
对于网络中的    卷积层  
conv2d = nn.Conv2d(1,1, kernel_size=(1, 2), bias=False)
从前往后分别为(输入通道数，输出通道数，卷积核大小，是否有偏置单元)

对于     输入到网络层中的四维输入变量  
X = X.reshape((1, 1, 6, 8))
Y = Y.reshape((1, 1, 6, 7))
从前往后为(批量大小、通道、高度、宽度)

此处的1是范例中XY仅仅是一个    只有一组数据(批量大小为1)的二维数组(通道为1)      作为范例。
XY批量大小和通道数为1，而不是指用一个数字来切换。
比如如果有两个通道，那么就会先一个二维数组，在后来一个二维数组。而非在从右往左第三位数字迭代。
所以当数据是2个通道的数据那么从右往左第三个数字会是2
当数据有好4个同时作为一个批量大小放入的时候，从右往左第四个数字会是4


变量本身的通道数在输入某个层时要与该层的输入通道数对应
如上面的conv2d的第一个1和X中的第二个1对应

具体代码见62
'''

'''
对于输入通道来说
单通道和多通道一样，只不过要在入口对输入通道数做匹配，相当于只是改个数然后网络训练变得复杂点
会多训练点参数
'''


'''
*layers
意思为解包layer
表示取layer中的内容放到nn.Sequential中
即把我们设置的各个层每次append到layer这个数组中
然后按顺序取出来放到nn.Sequential里组成我们的VGG块

官方说法：把列表中的元素按顺序作为参数的输入函数
'''


'''
注：通道数翻倍，高宽减半(最后一层也可以翻倍成1024没关系，不过512很大一般不翻倍了)
这是一个经典的设计，之后会重复出现
'''


'''
问题3：为什么最后NiN只有一个全局池化层而不用softmax了
答：我们有softmax，这个过程是封装在我们training函数（train_ch6）中的,是在计算loss那里做的，没有放在网络上面，所以看不到
'''

'''
            这里要求的是      
            每个样本中每一个特征的均值，然后拼出一个X形状的mean
            mean.shape = X[0].shape
            见75test文件
            
            mean的dim=0是指   按行求均值
            即对每列求均值
            是计算特征维的均值
            是一个1Xn的行向量
            
            见https://blog.csdn.net/Apikaqiu/article/details/104379960
'''
'''
卷积层BN


从75test的结果可以看出
mean2 = X.mean(dim=(0, 2, 3), keepdim=True)
先把dim3平均，dim3上一个数
此时在dim2上就变成两个数，然后在平均
dim2上变成一个数

dim3，4上此时就是    横着3个数，竖着四行

再在dim4上平均
按列，竖着的都合并

最后变成3个数
形状torch.Size([3])

如果keepdim那么形状就torch.Size([1, 3, 1, 1])
从抽象理解就是仅保留我们剩下的做平均的维度，其余为1

我们要在哪个维度做平均，我们就在其余维度mean(dim=)
'''