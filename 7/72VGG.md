## tips:VGG设定时，每个VGG块中所有的卷积层个数和通道数是超参数，

## ==每个VGG块内的所有卷积层通道数一样==

## ==多个VGG块构成网络的一部分，不同VGG块中的卷积层通道数可以不同==

![image-20220901143409321](D:\论文\截图\image-20220901143409321.png)





# 为什么VGG

AlexNet没有提供==一个通用的模板==来指导后续的研究人员设计新的网络。

我们将介绍一些常用于设计==深层神经网络的启发式概念==。

研究人员开始从单个神经元的角度思考问题，发展到整个层，==现在又转向块，重复层的模式==。

使用块的想法首先出现在==VGG网络==中。

通过使用==循环和子程序==，可以很容易地在==任何现代深度学习框架的代码中==实现这些重复的架构。

![image-20220831201940568](D:\论文\截图\image-20220831201940568.png)

==现在仍然在一个让网络更深更大的过程中==

更多的全连接层——太贵

更多的卷积层——AlexNet卷积层做的不是那么好，把LeNet弄大然后加三个卷积层。即如果要加新的难道把3X3卷积再加十个？这样不好



所以VGG的思想——==把卷积层组合成块，再把卷积块摞上去==

# VGG和AlexNet的对比

==VGG是AlexNet思路的拓展==

![image-20220831202653222](D:\论文\截图\image-20220831202653222.png)

AlexNet连续三个一样的卷积层最后加一个3X3池化层（步长为2，使网络大小减半），三个3X3的卷积层，==输出通道数==为384，pad为1（保持网络大小不变）



VGG：

有==n层==（n作为一个超参数）3X3的卷积层，pad为1（保持网络大小不变）

其中每个VGG块中每个卷积层的==输出通道==为==m个通道==（m作为一个超参数）

最后是一个==2X2的池化层==（步长为2，使网络大小减半）



VGG的思想：一个VGG块我堆大量3X3卷积层，多个VGG块组成网络的一部分



对于为什么选用3X3而不是5X5？

答：5X5卷积层计算量大于3X3，网络架构会浅一点（显卡计算容量有限）

在相同的显卡计算的开销下，更多层3X3卷积层计算的效果优于相对较少层的5X5卷积层计算。



# VGG架构

![image-20220831203925605](D:\论文\截图\image-20220831203925605.png)

多个VGG块后接全连接层

==用VGG块替换掉整个AlexNet架构中的一堆卷积层==

VGG-16就是13个块加三个全连接层，VGG-19就是16个块加三个全连接层

# 总结

![image-20220831205157772](D:\论文\截图\image-20220831205157772.png)

![image-20220831205431028](D:\论文\截图\image-20220831205431028.png)

奠定了思想：1、卷积块

2、不同卷积块个数和超参数使得每种网络有不同的复杂度（高配版低配版）