## tips:BN作用其实==即固定一个batch的数据在每一层的feature map的均值和方差==。那么相对来说他就是比较稳定的。（上层在重新学习时候不会因为差很多而进行特别长时间的梯度下降，而是只要改一点点就够了）。解决了上面学得快下面学的慢，结果下面学了改完又要动上面这种情况。

## tips：ResNet论文精读里面会有这个的详细解释，可以先粗看然后回过头来补

## tips:加快收敛速度的原因==也是==因为==加了BN可以调大学习率==。即==使得每一层输入输出都在一个范围内==（即因为减去均值除以方差了）（虽然还有$\gamma$和$B$来改变大小），这样可以==调大学习率也不会梯度爆炸==。因为加入了BN以后，上层不会因为学习率大了然后梯度又大（上层的输入输出较大）爆炸炸掉，下层不会因为学习率小了然后梯度又小（下层的输入输出较小）基本不学习。（听说有言论说BN把梯度变得非常的光滑，学起来快）



# 批量归一化BatchNormalization

特定的层是在16年出来的

![image-20220906142141889](D:\论文\截图\image-20220906142141889.png)

当我们网络特别深的时候，从右图来看

我们输入的==数据==在最下面，我们的==损失==在最上面

存在的问题：

反传过程中，往回传梯度一般是小数。越到下面乘的越多梯度越小。

上层梯度大，越到下面（接近数据）梯度越小。底部层学习慢，底部变化所有层都得变化导致上面学的快的层又需要重新学习，导致收敛变慢。



但是，对于底部一般都是抽取一下图片底层的纹理信息，而上层则是把这些纹理信息融合并抽取较高层的语义信息。

导致：在训练持续进行的过程中，顶部变化快，底部变化慢。底部在不断改变时候，顶部也在不断重新训练来拟合底部的变化所导致的问题。导致收敛变慢。

![image-20220906143420541](D:\论文\截图\image-20220906143420541.png)

![image-20220906143441931](D:\论文\截图\image-20220906143441931.png)

为什么上层会根据下层改变，是因为每层的均值和方差会在每层输出进行改变

那么我们把feature map的分布固定了，即==每一层的输出、梯度都服从一个分布==，那么相对来说他就是比较稳定的。（上层在重新学习时候不会因为差很多而进行特别长时间的梯度下降，而是只要改一点点就够了）



==即固定一个batch的数据在每一层的feature map的均值和方差==



符号的解释：（以全连接层举例说明）

$x_i$：一个全连接层的向量的feature map

B：进行BN的小批量的索引

$\mu_B$:小批量的均值

$\sigma_B^2$:小批量的方差

$\gamma$:BN后的方差（可学习参数）

$\beta$：BN后的均值（可学习参数）

$x_{i+1}$:BN计算后的向量的feature map

防止变成0故有$\gamma \beta$(会被限定住不会变化的过于猛烈)

![image-20220906144908144](D:\论文\截图\image-20220906144908144.png)

作用在输出上或者输入上，作用在输出上时会在激活函数前。



对于全连接层作用在特征维

对于输入的二维数组，每一行是样本（一个个特征数组），每一列是特征。

BN作用于列



对于卷积层作用在通道维

类似1X1卷积上，1X1的卷积核就是对某个部分的块块的所有通道进行类似全连接的互相关操作。

如下图==浅蓝色方块==

![image-20220713222910469](D:\论文\截图\image-20220713222910469.png)

对于一个像素，有一个很多通道。那么这==几个通道可以看做一个向量==，这个==多个通道==看做的==向量==就可以看做这个==像素的特征==。

对于一个输入高宽的卷积层输入，==每个像素就是一个样本==。

对于一个卷积层，假设输入大小是   ==批量大小 X 高 X 宽 X通道数==，那么==样本数==就是==批量大小 X 高 X 宽==



# 批量归一化具体在做什么

![image-20220906150120267](D:\论文\截图\image-20220906150120267.png)

最初作者是用来==减少内部协变量的转移==

即数据会随时间变化，比如疫情前的对于人口聚集的模型在疫情时就可能不适用。

所以要保证模型在之后也能用，于是由此减少内部协变量转移

批量归一化认为其可以减少不同层之间的covariate shift（协变量转移）



但是后续论文计算了内部的协变量说并没有



![image-20220906155228409](D:\论文\截图\image-20220906155228409.png)

==噪音：$\mu_B 和 \sigma_B$==

其是从每个==随机的==小批量上计算而来的，==随机偏移和随机缩放==。

其是当前的均值和方差，==其噪音是非常大的==，将其一减一除做了随机的变量的转移



首先加了一个随机的偏移和缩放，然后==通过一个学到的比较稳定的均值和方差$\gamma 和 \beta$==

来==1、变化不那么剧烈。2、存在一定的随机性。==



结论：==BN可以看做在每个小批量里面加入噪音来控制复杂度的方法==



BN已经控制了模型复杂度，此时再加一个drop out层就没有什么必要了



# 总结

![image-20220906160123795](D:\论文\截图\image-20220906160123795.png)

可以加速收敛，但不改变模型精度



