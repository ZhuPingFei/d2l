# 含有并行连接的网络GoogleNet

通过前面几节课我们会去想是1X1卷积、3X3卷积、5X5卷积、最大池化层、两个1X1卷积层哪个最好，googlenet就把所有的全部都用上了。

# Inception块

![image-20220904111513820](D:\论文\截图\image-20220904111513820.png)

在块中抽取了不同通道。



==在Inception块中，输入输入的形状一样，变的是输出的通道数==



输入是input，输出是concatenation

输入被复制成四块

1、输入先接入到一个1X1的卷积层中。（即不识别空间信息，仅对通道做变换）

2、先通过1X1卷积层对通道做变换，再输入到3X3卷积层，pad等于1使得输入输出形状大小一样

3、先通过1X1卷积层对通道做变换，再输入到5X5卷积层，pad等于2使得输入输出形状大小一样

4、先通过一个3X3的最大池化层，pad=1，后面再接一个1X1的卷积层



四条路都没有改变高宽，最终完成后进行一个==concat操作==把他们在==输出通道上做一个合并==。

（即各自输出多输出通道的结果，并且各自在输出通道上合并）

![image-20220904131607065](D:\论文\截图\image-20220904131607065.png)

第一个1X1卷积层直接把通道数压到64

第二个先通过1X1卷积层把通道数压到96==降低通道数来控制模型复杂度==

模型复杂度一定程度上可以认为是模型可以学习的参数的个数

卷积的模型可学习的参数个数就是   ==输入通道数  X  输出通道数  X  卷积核的高   X  卷积核的宽==

然后通过通过3X3卷积层把通道数加到128

第三个更贵，（5X5会使得乘以通道数后模型复杂度更大）所以分配更少的通道数

第四个最大池化层，池化层本身不会改变通道数，又因为3X3的池化核和pad1放一起不会改变高宽

然后接一个1X1改变输出通道数





==上图中，标出白色的块是用于改变通道数目的（改变下一层的输入或者上一场的输出），标出蓝色的块是用于抽取信息的==

第一个只抽取通道信息不抽取空间信息，第二第三第四抽取了空间信息，第四个最大池化抽取空间信息使得预测更加的鲁棒。



数字的设计没有特别的说法，从结果解释上来说，==假设我们有规定的输出通道数==

我们==每个通道==差不多意思就是去识别一些==特定的模式==，要把==更多的通道数留给我们认为比较重要的通道==

一半的留给3X3卷积，剩下的一半里，一半给1X1卷积——不看空间信息只看通道信息。在剩下的对半分

那么对于第二条第三条前面的1X1卷积，基本就是将其3/4或减半。

![image-20220904134005547](D:\论文\截图\image-20220904134005547.png)

Inception块有大量1X1卷积，参数个数和计算复杂度低的多。

计算和参数个数基本是线性关系

# GoogLeNet

![image-20220904134212362](D:\论文\截图\image-20220904134212362.png)

分成5个stage（==每个stage代表一次高宽减半==）

注：从下往上第一个1X1的卷积层被视为非常重要，其降低了通道数来给3X3卷积层增加通道数做铺垫。



每次降低高宽用的就是stride等于2的最大池化层，每次的卷积和Inception块是不降低高宽的。



==第五个stage最后使用全局平均池化层把高宽变为1X1，此时通道数并不与种类数一一对应==

全局平均池化后flatten展平，出来的==数组==为形状为==（批量大小，通道数）==

然后该==数组==作为==FC==（fully connected layer==全连接层==）的输入

全连接层的输出大小为==种类数==

即最后通过全连接层来映射，更加的灵活。

# 每个block的设计

![image-20220904143521837](D:\论文\截图\image-20220904143521837.png)

在stage1和stage2

GoogLeNet和AlexNet的对比

第一个卷积层的卷积核和stride变小了，输出通道数减小

==高宽减小的幅度变小了==



1X1的卷积相比前面，通道数没有改变，即做一下多通道特征提取的融合。



GoogLeNet差不多把高宽减小8倍，通道数提高。

GoogLeNet和AlexNet都是要在这里把==高宽减下去把通道数拉起来==，使得后面的计算==可控==。只是GoogLeNet高宽保留的更多

（如果高宽很大，那么后面的输入高宽也会很大，那么计算量很大）

GoogLeNet高宽保留的更多可以==支撑后面使用更深的网络==

![image-20220904144328281](D:\论文\截图\image-20220904144328281.png)

第三个stage使用了两个Inception块

通道数目由192到256到480

两个Inception块==通道数分配比例==是不同的。

在第二个中3X3没有拿到一半的通道数

![image-20220904144959404](D:\论文\截图\image-20220904144959404.png)

stage4和stage5   

# Inception后续变种

![image-20220904150330227](D:\论文\截图\image-20220904150330227.png)

v2加上BN，batch normalization批量归一化，下一节课讲

v3修改了块的形式

v4使用了残差连接（resnet的模式），下下节课讲

![image-20220904150636310](D:\论文\截图\image-20220904150636310.png)

把5X5改成两个3X3

![image-20220904150702204](D:\论文\截图\image-20220904150702204.png)

把3X3换成1X7和7X1

把5X5换成两个1X7和7X1

![image-20220904150806167](D:\论文\截图\image-20220904150806167.png)

如图

这种思路就是==先进行横向的特征检测，后进行纵向的特征检测==

![image-20220904150919746](D:\论文\截图\image-20220904150919746.png)

Inception的特点，精度高、计算慢、耗内存。

![image-20220904151256344](D:\论文\截图\image-20220904151256344.png)





















