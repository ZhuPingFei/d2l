## tips：==全局平均池化层==，几篇网络叠着，每一片都是代表一个类别的表示。对每一片做全局平均池化，得到每一片的数。即每个类别得到一个数，然后softmax。所以其输入通道就是类别数

# NiN

LeNet、AlexNet和VGG都有一个共同的设计模式：通过一系列的==卷积层与池化层==来提取==空间结构特征==；

然后通过==全连接层==对特征的表征进行==处理==。

 AlexNet和VGG对LeNet的改进主要在于如何==扩大和加深==这两个模块。 或者，可以想象在这个过程的==早期使用全连接层==。

然而，如果使用了==全连接层，可能会完全放弃表征的空间结构==。 



网络中的网络（==NiN==）提供了一个非常简单的解决方案：==在每个像素的通道上分别使用多层感知机==



用自己话说：全连接层直接展平了网络，丢弃了网络的空间表达，NiN解决了这一点。



![image-20220901203135146](D:\论文\截图\image-20220901203135146.png)

卷积层的参数：

输入通道数 X 高（卷积核） X 宽（卷积核）X 输出通道数



第一个全连接层的参数：

输入是展平后的值，就是       上一个卷积层的输出通道数 X 高（网络） X 宽（网络）

输出是全连接层规定的值



全连接层的参数就是     输出乘以输入，==全连接层需要的参数会很多==

==特别是卷积层结束后的第一个全连接层==



参数多的缺点：1、用很多的内存。2、占用很多的计算带宽。（不断访问矩阵）3、很容易过拟合。（参数多就过拟合了）



所以==NiN干脆不用全连接层==。

# NiN块

![image-20220901204629709](D:\论文\截图\image-20220901204629709.png)

一个卷积层后跟==两个全连接层（1X1的卷积层）==。

 ![image-20220901204947195](D:\论文\截图\image-20220901204947195.png)

交替使用NiN块和步幅为2的最大池化层

全局平均池化层（池化层的核的高宽和网络高宽一样大），即把==每个通道整个输出的平均值==拿出来。



==具体方法==：1、需要分几类则在最后的全局平均池化层==输入通道数==为其==类别数==

2、每个通道整个输出的平均值是一个值，==这个值==作为这个通道也就是对这个==类别的预测==

3、对于这些类别的预测的值加上softmax就会得到识别为某一类的概率。



##  tips：即==全局平均池化层==，几篇网络叠着，每一片都是代表一个类别的表示。对每一片做全局平均池化，得到每一片的数。即每个类别得到一个数，然后softmax。所以其输入通道就是类别数



# NiN与VGG的对比

![image-20220901205617320](D:\论文\截图\image-20220901205617320.png)

NiN网络就是       

==先是一个NiN块，然后是一个最大池化层，然后不断重复这个过程，直到最后NiN块使得输出通道数为类别数，这样全局池化层的输入通道就是类别数。末尾为一个全局平均池化层，获得对于每个类别的预测的概率。==

（如果最后NiN没有使得全局池化层的输入通道数为类别数，则需要多一个展平和一个全连接层来将获得每个类别的输出在进行softmax）



所以其实NiN取代了之前VGG中前面的VGG块（获得特征）和前面的全连接层（处理卷积层获得的特征）。

全局平均池化层取代最后一层全连接层来实现分类

# 总结

![image-20220901205858999](D:\论文\截图\image-20220901205858999.png)

卷积层后面的==两个1X1的卷积层==来增加了==非线性性==

==（评论中：是因为每次后面加了ReLU？）==





所以其实NiN取代了之前VGG中前面的VGG块（获得特征）（处理卷积层获得的特征）

全局平均池化层取代全连接层（处理卷积层获得的特征来实现分类）

==这样不容易过拟合，更少的参数个数==





整体来说NiN架构比较简单，参数个数少（整个没有全连接层）