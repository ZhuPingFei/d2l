![image-20220829152602291](D:\论文\截图\image-20220829152602291.png)

1、当时算力不够，神经网络不能跑的很深。

2、核方法在理论上有更加完备的解释

![image-20220829163147167](D:\论文\截图\image-20220829163147167.png)

不符合

我们的优化目标就是选取特定的特征使得最后一层能够分类，不需要保证人能读懂。

![image-20220829201026106](D:\论文\截图\image-20220829201026106.png)

李沐回答不了

![image-20220829202528714](D:\论文\截图\image-20220829202528714.png)

比如LeNet最后就是一个高斯层

这个其实就是normalization，被证明没有太大用。

除了AlexNet之后就再也没有别的网络用过了，之后会有更多的normalization来给我们讲。

比如：batch normalization

![image-20220829202833510](D:\论文\截图\image-20220829202833510.png)

一个不行，一个效果会差。

==经验：两个4096X4096会是非常厉害的模型==

因为在AlexNet中，前面的卷积抽取特征不是那么好那么深，所以需要两个比较大的全连接层（Dense）来补

![image-20220829203758367](D:\论文\截图\image-20220829203758367.png)

保证高宽比，并且每次从里面扣出一块适合网络形状的方形

![image-20220829203929243](D:\论文\截图\image-20220829203929243.png)

之后会解答