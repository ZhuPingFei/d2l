![image-20220908130452473](D:\论文\截图\image-20220908130452473.png)

加更多的层能改善精度嘛？不一定



每一个小块都是一个模型（函数类），其大小代表了其复杂程度

其覆盖的区域表示这个模型可以拟合出来的对应   ==函数==   的范围

越复杂则大小越大，可以拟合出来的的模型（函数类）的范围越大



五角星代表我们问题的对应精确的   ==函数==   的的那个点

那么我们使用的模型会学习到  其可学习成为的所有   ==函数==   中最接近五角星的那一个

但是不是模型（函数类）越大离五角星最近的那个点就最近（如图一）

![image-20220908132002240](D:\论文\截图\image-20220908132002240.png)

那么应该如何做呢？

如图二

每次更复杂的模型包含前一个子模型

至少每次不会更差

resnet就是加更多的层，每次都是在包括原有函数类的基础上扩大现有函数类

至少使得模型预测不会变得更差

通常是变更好

# 残差块

![image-20220908132237214](D:\论文\截图\image-20220908132237214.png)

即中间那个  卷积层-激活层-卷积层  是我们新加的块

抽象解释：即使==这个什么也没有学到==，那么在resnet中，我们会有一个快速通道可以绕过这个层，保留着上层的结果供给后面的网络使用

不会使得模型函数类偏移导致有不包括上一层的函数的情况。相当于这个网络可以使得在每一层计算时，嵌入了前一层小一点的网络，使得模型先去拟合小网络



确切解释：这个快速通道可以把残差传过去过去使得梯度更快。



# ResNet块细节

![image-20220908134106345](D:\论文\截图\image-20220908134106345.png)

灵感来源于VGG，使用多个3X3卷积层

第一种就是通过一个块后加上原先的输出



第二种就是通过一个块后，先通过1X1卷积变换通道然后加到块的后面

（假设块中的卷积层要对通道数做变换，则原本的x加不回去，所以要有1X1卷积层做通道变换）



# 不同的残差块

![image-20220908135158742](D:\论文\截图\image-20220908135158742.png)

可以尝试x放到不同的位置

从实验来看，各种的准确率都差不多，甚至原始的更好一些

# ResNet块

![image-20220908135409421](D:\论文\截图\image-20220908135409421.png)

高宽减半的ResNet块

第一块中第一个卷积层步幅为2，后接==多个==高宽不变的ResNet块（步幅为1），则把高宽减半



# ResNet架构

![image-20220908135705533](D:\论文\截图\image-20220908135705533.png)

通过不同的ResNet块的==数量和不同的输出通道数==

可以得到不同的ResNet架构



类似GoogLeNet，5个stage。但是用ResNet块



基本上第一个stage所有架构都是7X7卷积和3X3maxpooling

后面就是自己想要的样子



ResNet一般用来比赛，现实中用的很少，太贵了

一般34最多，再不行上50

101、152用的很少

![image-20220908140608120](D:\论文\截图\image-20220908140608120.png)

# 总结

![image-20220908140647223](D:\论文\截图\image-20220908140647223.png)

残差块使得很深的网络更加容易训练

原因：再深，因为连接层的存在，大网络总是可以包含住小网络。总是会先把模型不复杂的那些网络先行训练好，然后再去训练更深的网络。





































