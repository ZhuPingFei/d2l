![image-20220906133957306](D:\论文\截图\image-20220906133957306.png)

本质上没有区别，都是用来增加模型稳定性的，即不让梯度爆炸或者梯度消失，一旦模型稳定了收敛就不会慢。



xavier是==选取比较好的初始化==，是对==权重==的，使得在初始的时候下落比较稳定

BN是指，在整个训练的过程中我都强行的在每个层后面或者前面做归一化——把==数据（feature map）==变成均值为0方差为1的数据

注：归一化和统计上的normalization不一样，这里是被混淆了



之前讲初始化时候，我们觉得这样在满足这个假设的情况下也能均值0方差1的分布使得模型比较稳定，核心是让数值比较稳定



![image-20220906134944731](D:\论文\截图\image-20220906134944731.png)

不是，权重衰退是指在更新权重时候，每次把权重除以一个小值，把权重变得比较小。

批量归一化不会对前一层的权重有太多的影响。



（权重衰退就是L2范数，在后面加惩罚项限制权重w的取值范围。即对于我们限制w的范围$\theta$存在$\lambda$

使得原式在loss时加上$\frac{\lambda}{2}  { \|  \Bbb w  \| }^2    $ 使得在梯度下降时候可以限制w的大小）

![image-20220906140143683](D:\论文\截图\image-20220906140143683.png)

可以的，但主要对于深度的神经网络有用（==因为主要就是解决上层数值大，不好用大的学习率导致下层学得慢的问题。把数值搞在一个差不多大小的分布上==）

![image-20220906140413693](D:\论文\截图\image-20220906140413693.png)

是的，如上

![image-20220906140436415](D:\论文\截图\image-20220906140436415.png)

python的语法，即X.shape的长度必须在（2,4）这个元组之中（即是2或者4），不然报错

![image-20220906141343925](D:\论文\截图\image-20220906141343925.png)

一般调的话，顺序是epoch、batchsize、学习率。即如果看到收敛了，epoch可以小一点停在已经收敛的那个地方。

一开始调试的时候epoch数可以大一点batch size合适些不要太大太小，太大炸内存。最后调学习率，保证效率的情况下不要梯度爆炸和梯度消失。

框架一般不换。



