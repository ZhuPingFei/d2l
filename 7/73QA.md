问题1、pytorch或者mxnet写好的模型如何上线（变为工程），用C++重写？

答：假设线上一定要用C++的话，可以用torchscript转到C++

取决于模型是不是很复杂，如果过于复杂，torchscript不一定能获取



mxnet如果用的Imperative Programs要用hybridize得到计算图，得到计算图后就可以通过C++的backend



tensorflow本身可以拿到计算图，也有C++部署方案



或者第三方的解决方案，转成onnx模型。或者用pbm工具转过去到C++端。





问题2

serving的解决方案（一个服务来提供网络的功能）

答：李沐讲了，但是之后自己要用了在积累吧





==问题3：为什么最后只有一个全局池化层而不用softmax了==

答：我们有softmax，这个过程是封装在我们training函数（train_ch6）中的是在计算loss那里做的，没有放在网络上面，所以看不到。



==问题4：全局平均池化层的设计思想==

答：后面带来了非常大的影响。即提供了一个思路，==我可以每次在卷积层的后面加全局平均池化层==，把卷积的高宽压成1，把输入减小了并且其本身有可学习的参数。

让计算变简单，把模型复杂度降低。

大家发现加入之后会==提升模型的泛化性==，被大量的使用。



坏处：让收敛变慢。因为把所有东西变小了，之后的输入到dense层 输入到全连接层的东西变小了。

比如AlexNet和VGG只要50轮左右，但是后续的加入全局平均池化层的网络一般都要训练120轮左右。



问题5：为什么NiN块选用两个而不是一个或三个

答：试出来的



==问题6：两个1X1卷积层为每个像素增加了非线性性？==

答：1X1卷积层的作用：==对每个像素其对应的通道的那个向量   做了一个全连接层。然后把这个全连接层对每个像素作用一遍。==



用了两个1X1的卷积层意味着什么，==对每个像素他的输入通道数做了一个有两个隐藏层（输入层输出层中间的就是隐藏层）的MLp==

（即原本1X1卷积层提供的是一个类似全连接层的作用，两个1X1卷积层做的就是类似一个有两个隐藏层的全连接层的作用）



MLP中间有ReLU函数，不管一层两层都增加了非线性。



==问题7：如何用我们写的模型来进行预测的测试？==

答：用3.6节softmax从零实现那里的predict_ch3

![image-20220903174925092](D:\论文\截图\image-20220903174925092.png)

看看有没有predict_ch6，没有就用这个

区别就是x得copy到GPU上，然后predict拿出来后要copy回CPU



（有人说有的，在lenet那章的evaluate_accuracy_gpu函数调用了d2l.accuracy在gpu上做predict）





问题8：pytorch网络权重会自己初始化嘛

答：会，但是我们在train函数中有自己来初始化一遍，这是为了可控性