一篇特别好的知乎文章，讲的很细

https://zhuanlan.zhihu.com/p/427541517

![image-20220717000439811](D:\论文\截图\image-20220717000439811.png)

![image-20220717000452556](D:\论文\截图\image-20220717000452556.png)









激活函数是[神经网络](https://so.csdn.net/so/search?q=神经网络&spm=1001.2101.3001.7020)的重要组成部分，其作用主要有两点：

- 由于卷积和全连接均是线性操作，多层叠加仍然是线性，不具备非线性建模能力。在网络中加入激活函数这一组件能够引入非线性，使网络具备非线性建模能力。
- 部分激活函数如Sigmoid、tanh能够将特征值映射到某一范围内。





如果不用激励函数（其实相当于激励函数是f(x) = x），在这种情况下你每一层节点的输入都是上层输出的线性函数，很容易验证，无论你神经网络有多少层，输出都是输入的线性组合，与没有隐藏层效果相当，这种情况就是最原始的感知机（Perceptron）了，那么网络的逼近能力就相当有限。正因为上面的原因，我们决定引入非线性函数作为激励函数，这样深层神经网络表达能力就更加强大（不再是输入的线性组合，而是几乎可以逼近任意函数）。
