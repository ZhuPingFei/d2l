1、dropout 随机置0对求梯度和反向传播的影响是什么？

如果被随机置0，则梯度也会变0

没有置0的地方，梯度会增加



相当于dropout的那些数有关的  权重  在这一轮不会被更新

从概念上就是   每一次选取一部分神经网络进行更新



2、dropout一般就是0.1、0.5、0.9

比如一个64隐藏层用着挺好，没有过拟合

那就开一个128隐藏层然后0。5dropout