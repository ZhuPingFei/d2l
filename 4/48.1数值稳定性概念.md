# 数值稳定性

![image-20220619171727214](D:\论文\截图\image-20220619171727214.png)



t表示第几层

$h^{t-1}$是  t-1  层的隐藏层的输出，也是 t 层的输入

经过一个$f_t$ 得到$h^t$ 也就是 t 层的输入



y 表示的是整个神经网络的优化目标，即从$f_1$到$f_d$ 得到输出后做一个损失函数$\Bbb l$

（同权重衰退那一章节一样，用这个符号表示损失函数）

![image-20220619172316582](D:\论文\截图\image-20220619172316582.png)

即里面是相关参数。



==那么  损失  关于   某一层的参数w的   梯度就是==

![image-20220619172439674](D:\论文\截图\image-20220619172439674.png)

由链式法则，l 函数对最后一层隐藏层求导，然后每层对前一层求导，然后该层对w求导。

（最开始是  损失函数对于输出层输出量的偏导）（损失函数是输出量的函数）

（中间每一层都是相对于前一层的输出量（该层输入量）的偏导）（每一层的输出都是上一层的输出（这一层的输入）的函数）

（最后是w对应层的输出（一个有关w的函数）对于==权重==w的偏导）





==所有的h都是向量，向量关于向量的导数是==     ==矩阵==

共有1-t次的矩阵乘法



做了太多次矩阵乘法，会导致梯度爆炸和梯度消失

![image-20220620063127198](D:\论文\截图\image-20220620063127198.png)

---------------



举个例子：

![image-20220620074108709](D:\论文\截图\image-20220620074108709.png)

$h^{t-1}$是  t-1  层的隐藏层的输出，也是 t 层的输入

该函数可以表示成  权重和输入相乘，在激活函数的结果



第二行是求导

![image-20220620074307297](D:\论文\截图\image-20220620074307297.png)

先对外面的激活函数求导



激活函数是一个按元素的过程



==diag的意思是这是一个对角矩阵，里面的元素是对角的值==

$\sigma$ 函数是一个按元素的激活函数，$\sigma'$是激活函数的导数，其输入是一个==向量==，其输出也就是一个==向量==。用diag把这个向量包成一个对角矩阵。

 

后对里面的 h 求导，即本来就是对应相乘求和，对各个元素求偏导，只剩各个元素前面的分量，则合起来就是一个w向量（转置不考虑那么多，理解就好）



第三行就是累乘化开来，i 从t开始迭代到d-1

----------------



![image-20220620075054567](D:\论文\截图\image-20220620075054567.png)

使用relu激活函数，则其导数非0即1

使得那个对角矩阵的对角元素非0即1

所以我们所得到的式子的元素会来自于  ==w矩阵的累乘==



==w矩阵是各层的输入输出的权重，所以是可以合并累乘为一个输入输出的形状的矩阵的，但是数字会在累乘中变得特别大，梯度爆炸==

-------------



![image-20220620075508134](D:\论文\截图\image-20220620075508134.png)



使用16位浮点数计算更快，所以数字的限制在这里



---------------



![image-20220620075833674](D:\论文\截图\image-20220620075833674.png)



当使用sigmoid函数输入值较大时，梯度就会很小，导数变很小，多个小数相乘梯度会过小

![image-20220620080025123](D:\论文\截图\image-20220620080025123.png)

-----

![image-20220620080039341](D:\论文\截图\image-20220620080039341.png)

==当梯度变深时候，顶层相当于是一个小神经网络，顶层训练会很好，但是到下面可能几乎不动了==

==即无法让神经网络更深==



























# 自己的想法

如果是正向，假设每层只考虑 wh 

那么也就是多层的   w 累乘堆在对应层的输入h前

输入输出维度对上，最后是一个总输入和总输出的矩阵

但是里面的==数字==会梯度爆炸
