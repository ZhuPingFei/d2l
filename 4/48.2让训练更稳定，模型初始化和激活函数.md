# 让训练更稳定

![image-20220620080407945](D:\论文\截图\image-20220620080407945.png)

归一化，不管多大梯度都拉回来

梯度剪裁，大于一个数就让他停在那个数



我们这里主要讲

## 合理的权重初始化和激活函数

方法

![image-20220620081154389](D:\论文\截图\image-20220620081154389.png)

均值和方差保持一致，不管加多深都在可控范围



==（这里的意思是我们要设定神经网络达到这样的目的，而不是公式推导）==



==对于正向，输出看作随机变量，这样不会导致累乘起来最后输出太大梯度爆炸==

$h_i^t$  t是第t层输出，i 是第i个元素

这是一个标量，其期望为0，方差为a

==对于反向，梯度看作随机变量，这样不会导致累乘之后梯度趋近于0无法学习而梯度消失==





------

![image-20220620082158947](D:\论文\截图\image-20220620082158947.png)

初始位置选的不好，梯度过大，w就会特别容易梯度爆炸



我们之前一直用0 ，0.001的正态分布，这对小网络好用，但是不能保证深度神经网络

小网络怎么乘也不太会有事，但是对于深度神经网络可能太小或者太大

---



下面看看此时我们具体要满足什么条件才能使得==网络的        输出和梯度的        均值和方差            都是常数（稳定）==

![](D:\论文\截图\image-20220620084413657.png)



假设  权重   是   独立同分布

$w_{i,j}^t$  是第t层第i行第j列

对其做如下==假设==

![image-20220620083029075](D:\论文\截图\image-20220620083029075.png)

 各层均值为0，方差各层不同，用t下标区分

![image-20220620083231555](D:\论文\截图\image-20220620083231555.png)

当前层的输入和当前层的权重（两个做点积运算的量），互相独立。









w的形状为

![image-20220620084326695](D:\论文\截图\image-20220620084326695.png)

表示其连接的输入输出层的大小



![image-20220620084428598](D:\论文\截图\image-20220620084428598.png)

当前层的迭代是 i

前一层的迭代是 j

当前层的第 i 个就是前一层对所有  j  迭代（共$n_{t-1}$次）与  i  相关的w分量乘以输入（上一层输出）



累加可以拿出来，w和h独立可以分成两个期望



假设中

![image-20220620085016730](D:\论文\截图\image-20220620085016730.png)

为0



所以结果为0





故==我们要保证==

![image-20220620090213554](D:\论文\截图\image-20220620090213554.png)

每一层的输出的期望为0

----

![image-20220620085040137](D:\论文\截图\image-20220620085040137.png)

==方差为平方的均值减去均值的平方==

均值为0，均值的平方为0，后半项省略



n个项求和后平方，等于每个项平方加上交叉项

![image-20220620085236468](D:\论文\截图\image-20220620085236468.png)



这个加法拿出去，套均值，又可以分来成点积的期望，又可以打开。

交叉项见前面式子，互相独立，互相均值为0.

后半项为0



![](D:\论文\截图\image-20220620085948130.png)

化解后，因为期望为0，所以期望转方差就是

平方的期望等于方差



![image-20220620085545075](D:\论文\截图\image-20220620085545075.png)

固定为$\gamma_t$

所以就是对   j   的累加，则是上一层的迭代，形状为

$n_{t-1}$



最后，因为要输入输出的方差一致

所以==我们要保证==

![image-20220620085914157](D:\论文\截图\image-20220620085914157.png)

----

故保证

1、每一层的输出的期望为0

![image-20220620090213554](D:\论文\截图\image-20220620090213554.png)

2、![image-20220620085914157](D:\论文\截图\image-20220620085914157.png)

---



![image-20220620090415351](D:\论文\截图\image-20220620090415351.png)

反向也类似

---

![image-20220620090521233](D:\论文\截图\image-20220620090521233.png)

显然同时满足   输出 和 梯度  的方差==是不现实的==（除非输入层等于输出层）

所以用Xavier做了一个折中

所以==在w初始化时候==对于正态分布和均匀分布做了对应的初始化修改==标准差的值==（这个是各层w的初始化，输入输出是相对的，不是总网络的输入输出）

其值由神经网络形状控制

---



![image-20220620091007056](D:\论文\截图\image-20220620091007056.png)

（推导很简单，不写了）

刚刚是假设没有激活函数

这里用一个没用的==线性激活函数==来直观感受一下



==（之后非线性激活函数的解法就是用线性函数来逼近这个非线性函数）==

以达到目的

反向同理

![image-20220620091554463](D:\论文\截图\image-20220620091554463.png)



==结论：激活函数必须是一个y=x的函数==（但是激活函数不能是线性函数，所以会是一个个线性函数用==泰勒展开式==去逼近这个函数）

----

==上一个结论的拓展：非激活函数必须在一定范围内是与y=x同形式的函数==（用==泰勒展开式==）

![image-20220620092400109](D:\论文\截图\image-20220620092400109.png)

所以tanh、relu可以在0点附近近似的和y=x一致

所以这个可以用于激活函数



sigmoid函数就要转变一下

















# 总结

针对数值稳定性，对神经网络权重的

1、期望

2、方差

3、激活函数

==做了要求==
