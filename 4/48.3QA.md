1、NaN和Inf

inf学习率太大或者权重初始太大梯度爆炸

NaN一般就是除0了，比如梯度很小了，把梯度除了0

解决方法：合理的初始化权重（往小走）、选取合适的激活函数和学习率（不要太大）



2、权重独立同分布只是开始时候的对于其的设定，态取决于学习过程，不保证独立同分布



3、

![image-20220620155051158](D:\论文\截图\image-20220620155051158.png)

不会，数值是一个区间，拉到什么地方都没关系。

只是让数值在一个合理区间表达，使得硬件处理起来比较容易。

不管用什么区间做任何变化，不会影响模型的可表达性

、



4、

![image-20220620160155596](D:\论文\截图\image-20220620160155596.png)

epoch是扫完一次数据集做完一次前向和反向的过程

权重是在一次epoch中的for  xy  在iter中

即取一个batch_size时，进行正向反向然后更新



先对损失函数进行   l  = loss。。。。



然后   l.backward

反传求梯度



梯度保存在     param.grad（即w这种权重   类  本身的一个grad   变量  中）中



优化函数通过调用这个梯度来==对神经网络中所有权重==进行权重更新。

==这里只更新了一次，即一个学习率步长==

==一个epoch后也只是所有的一个学习率步长==

==所以要有多个epoch来学习更新==

==（这里batchsize是用的SGD小批量随机梯度下降的概念，即取一部分数据来作为全数据的代表，来产生梯度）==

（batch就是批，batchsize就是批量大小）





一次batch只涉及部分数据集，所以要好几个batch才算一次epoch完整的更新。

即本身就是要对每个数据都跑一遍更新，只是从逻辑上的一个个跑（一个个拿出来算）可以理解成一个大矩阵的跑（相当于一个epoch）

但是实操是每次拿出一部分（batchsize个）











