# 莫烦：

卷积神经网络不是对图片每一个像素的处理，而是对图片上每一个小块的处理。加强图片信息处理的连续性。

卷积过程中会丢失信息，所以保留原尺寸卷积并把压缩交给池化来处理信息。

![image-20220705223217482](D:\论文\截图\image-20220705223217482.png)

一般形式，图片-卷积-池化-卷积-池化-全连接-全连接-分类

# 子豪兄

## 三分钟走进神经网络

https://www.bilibili.com/video/BV1qb411P7JD?spm_id_from=333.999.0.0&vd_source=e43cd5da3fc2ddd47ce6a6422591bfd8

先把图片转化为像素矩阵

卷积的具体核是通过喂数据出来的，我们并不能知道具体的操作。

下采样池化就是在保持特征不变的情况下把   卷积后的feature map  缩起来。大大减小了计算量。

![image-20220708165906271](D:\论文\截图\image-20220708165906271.png)

池化

![image-20220708170314269](D:\论文\截图\image-20220708170314269.png)

![image-20220708170327400](D:\论文\截图\image-20220708170327400.png)

![image-20220708195632756](D:\论文\截图\image-20220708195632756.png)

## 卷积神经网络的工作原理

https://www.bilibili.com/video/BV1sb411P7pQ?spm_id_from=333.999.0.0&vd_source=e43cd5da3fc2ddd47ce6a6422591bfd8

卷积神经网络是一个黑箱，输入就是二维阵列图片，输出就是图片的分类

![image-20220708202539996](D:\论文\截图\image-20220708202539996.png)

![image-20220708203605283](D:\论文\截图\image-20220708203605283.png)

把所有负数换成0，就是==ReLU激活函数==

![image-20220708203818513](D:\论文\截图\image-20220708203818513.png)

![image-20220708203908132](D:\论文\截图\image-20220708203908132.png)

把==多个卷积核卷积并ReLU完再池化后的矩阵==拉成一列，构成全连接层的输入

![image-20220708204228161](D:\论文\截图\image-20220708204228161.png)







如果行列是可以互换的，就不建议使用卷积神经网络，比如一个表格的数据，地址和电话的列可以互换

![image-20220708205814126](D:\论文\截图\image-20220708205814126.png)

# 霹雳吧啦

https://www.bilibili.com/video/BV1b7411T7DA?spm_id_from=333.999.0.0&vd_source=e43cd5da3fc2ddd47ce6a6422591bfd8















