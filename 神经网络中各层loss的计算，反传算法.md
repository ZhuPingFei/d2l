# 一个思路：向前是乘以W矩阵，那么反过来就是乘以WT，所以同理在计算梯度时候就是     后面的值     乘以WT。所以每一层的损失就是后一层（的式子（一串相减的式子））    乘以WT  ？（梯度下降就是连带着激活函数和原有式子求导）（或者类似这种方法计算，因为损失是l1是相减，l2只是相减平方）



![image-20220629225541843](D:\论文\截图\image-20220629225541843.png)

![image-20220629225606512](D:\论文\截图\image-20220629225606512.png)

证明过程

![Image](D:\论文\截图\Image-16565145803453.png)

![image-20220629225640786](D:\论文\截图\image-20220629225640786.png)

![image-20220629225651878](D:\论文\截图\image-20220629225651878.png)